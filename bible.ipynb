{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor2tensor==1.6.6\n",
      "tensorboard==1.9.0\n",
      "tensorflow-gpu==1.9.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf bible\n",
    "mkdir -p bible/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/phil'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bible/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bible/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "\n",
    "@registry.register_problem\n",
    "class Bible(text_problems.Text2TextProblem):\n",
    "    \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "        return False\n",
    "\n",
    "  @property\n",
    "    def dataset_splits(self):\n",
    "        \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "        return [{\n",
    "            \"split\": problem.DatasetSplit.TRAIN,\n",
    "            \"shards\": 90,\n",
    "        }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "        }]\n",
    "\n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        with open('/home/phil/bible/trainer/input.txt', 'r') as inp and open('/home/phil/bible/trainer/output.txt', 'r') as out:\n",
    "        \n",
    "            for en_line in inp and hi_line in out:       \n",
    "                yield {\n",
    "                    \"inputs\": en_line,\n",
    "                    \"targets\": hi_line\n",
    "                }\n",
    "             \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_bible():\n",
    "    hparams = transformer.transformer_base()\n",
    "    hparams.num_hidden_layers = 2\n",
    "    hparams.hidden_size = 128\n",
    "    hparams.filter_size = 512\n",
    "    hparams.num_heads = 4\n",
    "    hparams.attention_dropout = 0.6\n",
    "    hparams.layer_prepostprocess_dropout = 0.6\n",
    "    hparams.learning_rate = 0.05\n",
    "    return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_bible_range(rhp):\n",
    "    rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "    rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "    rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "    rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bible/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bible/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bible/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bible/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='bible',\n",
    "    version='0.1',\n",
    "    author = 'phil',\n",
    "    author_email = 'phildani7@nith.ac.in',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Bible Translation',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch bible/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bible\r\n",
      "bible/__init__.py\r\n",
      "bible/setup.py\r\n",
      "bible/trainer\r\n",
      "bible/trainer/__init__.py\r\n",
      "bible/trainer/problem.py\r\n"
     ]
    }
   ],
   "source": [
    "!find bible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phil/.conda/envs/t2tj/bin:/home/phil/.conda/envs/t2tj/bin:/home/phil/anaconda3/bin:/home/phil/anaconda3/bin:/home/phil/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\r\n"
     ]
    }
   ],
   "source": [
    "! echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "! echo $PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bible\r\n"
     ]
    }
   ],
   "source": [
    "PROBLEM = 'bible'\n",
    "!echo $PROBLEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/phil'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bible/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bible/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "\n",
    "@registry.register_problem\n",
    "class Bible(text_problems.Text2TextProblem):\n",
    "  \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('/home/phil/bible/trainer/input.txt', 'r') as inp, open('/home/phil/bible/trainer/output.txt', 'r') as out:\n",
    "        for en_line, hi_line in zip(inp, out):\n",
    "            yield {\n",
    "                    \"inputs\": en_line,\n",
    "                    \"targets\": hi_line\n",
    "            }       \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "@registry.register_hparams\n",
    "def transformer_bible():\n",
    "  hparams = transformer.transformer_base()\n",
    "  hparams.num_hidden_layers = 2\n",
    "  hparams.hidden_size = 128\n",
    "  hparams.filter_size = 512\n",
    "  hparams.num_heads = 4\n",
    "  hparams.attention_dropout = 0.6\n",
    "  hparams.layer_prepostprocess_dropout = 0.6\n",
    "  hparams.learning_rate = 0.05\n",
    "  return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_bible_range(rhp):\n",
    "  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=/home/phil/bible/trainer \\\n",
    "  --problem=bible \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/phil'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/phil/trained_model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=/home/phil/trained_model\n",
    "echo $OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Importing user module trainer from path /home/phil/bible\n",
      "WARNING:tensorflow:From /home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensor2tensor/utils/trainer_lib.py:165: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f91b139f748>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/home/phil/trained_model', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7f92317d6780>}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7f91b1394bf8>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:ValidationMonitor only works with --schedule=train_and_evaluate\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Reading data files from /home/phil/t2t_data/bible-train*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 90\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'train'\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8180_512.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8180_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8180_512.top\n",
      "INFO:tensorflow:Base learning rate: 2.000000\n",
      "INFO:tensorflow:Trainable Variables Total size: 48308224\n",
      "INFO:tensorflow:Using optimizer Adam\n",
      "/home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-07-17 00:57:44.361639: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2018-07-17 00:57:44.517245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-07-17 00:57:44.517671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:02:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.74GiB\n",
      "2018-07-17 00:57:44.594794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-07-17 00:57:44.595214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 10.92GiB freeMemory: 10.58GiB\n",
      "2018-07-17 00:57:44.596760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2018-07-17 00:57:44.875383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-07-17 00:57:44.875424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2018-07-17 00:57:44.875429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
      "2018-07-17 00:57:44.875432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
      "2018-07-17 00:57:44.875744: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11586 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2018-07-17 00:57:44.958873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10619 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/phil/trained_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 8.330817, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /home/phil/trained_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.236078.\n",
      "INFO:tensorflow:Reading data files from /home/phil/t2t_data/bible-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 10\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8180_512.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8180_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8180_512.top\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-16-19:28:17\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-07-17 00:58:17.503987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2018-07-17 00:58:17.504053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-07-17 00:58:17.504059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2018-07-17 00:58:17.504076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
      "2018-07-17 00:58:17.504079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
      "2018-07-17 00:58:17.504233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11586 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2018-07-17 00:58:17.504307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10619 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "INFO:tensorflow:Restoring parameters from /home/phil/trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-16-19:28:39\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 8.218343, metrics-bible/targets/accuracy = 0.0021286062, metrics-bible/targets/accuracy_per_sequence = 0.0, metrics-bible/targets/accuracy_top5 = 0.002180949, metrics-bible/targets/approx_bleu_score = 0.95259666, metrics-bible/targets/neg_log_perplexity = -9.427774, metrics-bible/targets/rouge_2_fscore = 0.96170896, metrics-bible/targets/rouge_L_fscore = 0.95636743\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /home/phil/trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Reading data files from /home/phil/t2t_data/bible-dev*\n",
      "INFO:tensorflow:partition: 0 num_data_files: 10\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'eval'\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Using variable initializer: uniform_unit_scaling\n",
      "INFO:tensorflow:Transforming feature 'inputs' with symbol_modality_8180_512.bottom\n",
      "INFO:tensorflow:Transforming 'targets' with symbol_modality_8180_512.targets_bottom\n",
      "INFO:tensorflow:Building model body\n",
      "INFO:tensorflow:Transforming body output with symbol_modality_8180_512.top\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-07-16-19:28:47\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-07-17 00:58:47.778602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2018-07-17 00:58:47.778682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-07-17 00:58:47.778687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2018-07-17 00:58:47.778705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
      "2018-07-17 00:58:47.778708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
      "2018-07-17 00:58:47.778843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11586 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2018-07-17 00:58:47.778931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10619 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "INFO:tensorflow:Restoring parameters from /home/phil/trained_model/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Finished evaluation at 2018-07-16-19:29:10\n",
      "INFO:tensorflow:Saving dict for global step 10: global_step = 10, loss = 8.218343, metrics-bible/targets/accuracy = 0.0021286062, metrics-bible/targets/accuracy_per_sequence = 0.0, metrics-bible/targets/accuracy_top5 = 0.002180949, metrics-bible/targets/approx_bleu_score = 0.95259666, metrics-bible/targets/neg_log_perplexity = -9.427774, metrics-bible/targets/rouge_2_fscore = 0.96170896, metrics-bible/targets/rouge_L_fscore = 0.95636743\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /home/phil/trained_model/model.ckpt-10\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=/home/phil/t2t_data\n",
    "OUTDIR=/home/phil/trained_model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --t2t_usr_dir=./bible/trainer \\\n",
    "  --problem=bible \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_base \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --worker\n",
    "  --train_steps=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./bible/my.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bible/my.txt\n",
    "Jesus loves you\n",
    "Thank you heavenly Father"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n"
     ]
    }
   ],
   "source": [
    "!echo $MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Importing user module trainer from path /home/phil/bible\n",
      "WARNING:tensorflow:From /home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensor2tensor/utils/trainer_lib.py:165: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdc353ebef0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/home/phil/trained_model', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fdbd415f7f0>}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fdbcae19e18>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:decode_hp.batch_size not specified; default=32\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/phil/.conda/envs/t2tj/bin/t2t-decoder\", line 16, in <module>\n",
      "    tf.app.run()\n",
      "  File \"/home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
      "    _sys.exit(main(argv))\n",
      "  File \"/home/phil/.conda/envs/t2tj/bin/t2t-decoder\", line 12, in main\n",
      "    t2t_decoder.main(argv)\n",
      "  File \"/home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensor2tensor/bin/t2t_decoder.py\", line 190, in main\n",
      "    decode(estimator, hp, decode_hp)\n",
      "  File \"/home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensor2tensor/bin/t2t_decoder.py\", line 92, in decode\n",
      "    checkpoint_path=FLAGS.checkpoint_path)\n",
      "  File \"/home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensor2tensor/utils/decoding.py\", line 274, in decode_from_file\n",
      "    p_hp = hparams.problem_hparams\n",
      "AttributeError: 'HParams' object has no attribute 'problem_hparams'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=/home/phil/t2t_data\n",
    "OUTDIR=/home/phil/trained_model\n",
    "DECODE_FILE=/home/phil/bible/my.txt\n",
    "\n",
    "PROBLEM=bible\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_base\n",
    "\n",
    "\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATADIR \\\n",
    "  --problems=bible \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_base \\\n",
    "  --output_dir=/home/phil/trained_model \\\n",
    "  --t2t_usr_dir=/home/phil/bible/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
