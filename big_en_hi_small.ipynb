{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor2tensor==1.9.0\n",
      "tensorboard==1.10.0\n",
      "tensorflow-gpu==1.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf big_en_hi_small\n",
    "mkdir -p big_en_hi_small/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/phil'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing big_en_hi_small/trainer/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile big_en_hi_small/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing big_en_hi_small/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile big_en_hi_small/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "  'tensor2tensor'\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name='big_en_hi_small',\n",
    "    version='0.1',\n",
    "    author = 'phil',\n",
    "    author_email = 'phildani7@nith.ac.in',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='big_en_hi_small Translation',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch big_en_hi_small/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big_en_hi_small\r\n",
      "big_en_hi_small/__init__.py\r\n",
      "big_en_hi_small/setup.py\r\n",
      "big_en_hi_small/trainer\r\n",
      "big_en_hi_small/trainer/__init__.py\r\n"
     ]
    }
   ],
   "source": [
    "!find big_en_hi_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing big_en_hi_small/trainer/problem.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile big_en_hi_small/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "\n",
    "@registry.register_problem\n",
    "class Big_en_hi_small(text_problems.Text2TextProblem):\n",
    "  \"\"\"Predict next line of poetry from the last line. From Gutenberg texts.\"\"\"\n",
    "\n",
    "  @property\n",
    "  def approx_vocab_size(self):\n",
    "    return 2**13  # ~8k      edited by phildani7\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # generate_data will NOT shard the data into TRAIN and EVAL for us.\n",
    "    return False\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    \"\"\"Splits of data to produce and number of output shards for each.\"\"\"\n",
    "    # 10% evaluation data\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 90,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 10,\n",
    "    }]\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    with open('/home/phil/big_en_hi_small/trainer/input.txt', 'r') as inp, open('/home/phil/big_en_hi_small/trainer/output.txt', 'r') as out:\n",
    "        for en_line, hi_line in zip(inp, out):\n",
    "            yield {\n",
    "                    \"inputs\": en_line,\n",
    "                    \"targets\": hi_line\n",
    "            }       \n",
    "\n",
    "\n",
    "# Smaller than the typical translate model, and with more regularization\n",
    "#@registry.register_hparams\n",
    "#def transformer_big_en_hi_small():\n",
    "#  hparams=transformer.transformer_base()\n",
    "#  hparams.optimizer_adam_epsilon = 1e-9\n",
    "#  hparams.learning_rate_decay_scheme = \"noam\"\n",
    "#  hparams.learning_rate = 0.1\n",
    "#  hparams.learning_rate_warmup_steps = 4000\n",
    "#  hparams.initializer_gain = 1.0\n",
    "#  #hparams.num_hidden_layers = 6\n",
    "#  hparams.initializer = \"uniform_unit_scaling\"\n",
    "#  hparams.weight_decay = 0.0\n",
    "#  hparams.optimizer_adam_beta1 = 0.9\n",
    "#  hparams.optimizer_adam_beta2 = 0.98\n",
    "#  hparams.num_sampled_classes = 0\n",
    "#  hparams.label_smoothing = 0.1\n",
    "#  hparams.shared_embedding_and_softmax_weights = int(True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#  hparams.num_hidden_layers = 2\n",
    "#  hparams.hidden_size = 128\n",
    "#  hparams.filter_size = 512\n",
    "#  hparams.num_heads = 4\n",
    "#  hparams.attention_dropout = 0.6\n",
    "#  hparams.layer_prepostprocess_dropout = 0.6\n",
    "#  hparams.learning_rate = 0.05\n",
    "#  return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "#@registry.register_ranged_hparams\n",
    "#def transformer_big_en_hi_small_range(rhp):\n",
    "#  rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "#  rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "#  rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "#  rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phil/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:Importing user module trainer from path /home/phil/big_en_hi_small\n",
      "INFO:tensorflow:Generating problems:\n",
      "    big:\n",
      "      * big_en_hi_small\n",
      "INFO:tensorflow:Generating data for big_en_hi_small.\n",
      "INFO:tensorflow:Generating vocab file: ./t2t_data/vocab.big_en_hi_small.8192.subwords\n",
      "INFO:tensorflow:Trying min_count 500\n",
      "INFO:tensorflow:Iteration 0\n",
      "INFO:tensorflow:vocab_size = 17260\n",
      "INFO:tensorflow:Iteration 1\n",
      "INFO:tensorflow:vocab_size = 8070\n",
      "INFO:tensorflow:Iteration 2\n",
      "INFO:tensorflow:vocab_size = 8298\n",
      "INFO:tensorflow:Iteration 3\n",
      "INFO:tensorflow:vocab_size = 8267\n",
      "INFO:tensorflow:Generating case 0.\n",
      "INFO:tensorflow:Generating case 100000.\n",
      "INFO:tensorflow:Generating case 200000.\n",
      "INFO:tensorflow:Generating case 300000.\n",
      "INFO:tensorflow:Generating case 400000.\n",
      "INFO:tensorflow:Generating case 500000.\n",
      "INFO:tensorflow:Generating case 600000.\n",
      "INFO:tensorflow:Generating case 700000.\n",
      "INFO:tensorflow:Generated 789337 Examples\n",
      "INFO:tensorflow:Shuffling data...\n",
      "INFO:tensorflow:Data shuffled.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "  --t2t_usr_dir=/home/phil/big_en_hi_small/trainer \\\n",
    "  --problem=big_en_hi_small \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=/home/phil/t2t_data\n",
    "OUTDIR=/home/phil/big_en_hi_small/trained_model\n",
    "t2t_tmp=$DATA_DIR\n",
    "#rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --t2t_usr_dir=./big_en_hi_small/trainer \\\n",
    "  --problem=big_en_hi_small \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_base \\\n",
    "  --output_dir=$OUTDIR \\\n",
    "  --worker_gpu=2 \\\n",
    "  --train_steps=10000000 \n",
    "  #--sync=True\n",
    "  #--eval_steps=1000 \\ #--sync=True \\\n",
    "  #--log_step_count_steps=1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Importing user module trainer from path /home/phil/bible\n",
      "WARNING:tensorflow:From /home/phil/.conda/envs/t2tj/lib/python3.6/site-packages/tensor2tensor/utils/trainer_lib.py:165: RunConfig.__init__ (from tensorflow.contrib.learn.python.learn.estimators.run_config) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "When switching to tf.estimator.Estimator, use tf.estimator.RunConfig instead.\n",
      "INFO:tensorflow:schedule=continuous_train_and_eval\n",
      "INFO:tensorflow:worker_gpu=1\n",
      "INFO:tensorflow:sync=False\n",
      "WARNING:tensorflow:Schedule=continuous_train_and_eval. Assuming that training is running on a single machine.\n",
      "INFO:tensorflow:datashard_devices: ['gpu:0']\n",
      "INFO:tensorflow:caching_devices: None\n",
      "INFO:tensorflow:ps_devices: ['gpu:0']\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa761b95ef0>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_train_distribute': None, '_device_fn': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.95\n",
      "}\n",
      "allow_soft_placement: true\n",
      "graph_options {\n",
      "  optimizer_options {\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_steps': 1000, '_keep_checkpoint_max': 20, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/home/phil/trained_model', 'use_tpu': False, 't2t_device_info': {'num_async_replicas': 1}, 'data_parallelism': <tensor2tensor.utils.expert_utils.Parallelism object at 0x7fa7009607f0>}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function T2TModel.make_estimator_model_fn.<locals>.wrapping_model_fn at 0x7fa6f6cdce18>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:decode_hp.batch_size not specified; default=32\n",
      "INFO:tensorflow:Performing decoding from a file.\n",
      "INFO:tensorflow:Getting sorted inputs\n",
      "INFO:tensorflow: batch 1\n",
      "INFO:tensorflow:Decoding batch 0\n",
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Setting T2TModel mode to 'infer'\n",
      "INFO:tensorflow:Setting hparams.dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.layer_prepostprocess_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.symbol_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.attention_dropout to 0.0\n",
      "INFO:tensorflow:Setting hparams.relu_dropout to 0.0\n",
      "INFO:tensorflow:Beam Decoding with beam size 4\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2018-07-17 02:03:12.011033: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2018-07-17 02:03:12.124554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-07-17 02:03:12.124983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties: \n",
      "name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:02:00.0\n",
      "totalMemory: 11.91GiB freeMemory: 11.74GiB\n",
      "2018-07-17 02:03:12.193505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2018-07-17 02:03:12.193958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 1 with properties: \n",
      "name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582\n",
      "pciBusID: 0000:01:00.0\n",
      "totalMemory: 10.92GiB freeMemory: 10.57GiB\n",
      "2018-07-17 02:03:12.194738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0, 1\n",
      "2018-07-17 02:03:12.480268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2018-07-17 02:03:12.480308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0 1 \n",
      "2018-07-17 02:03:12.480313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N Y \n",
      "2018-07-17 02:03:12.480316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 1:   Y N \n",
      "2018-07-17 02:03:12.480589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 11586 MB memory) -> physical GPU (device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1)\n",
      "2018-07-17 02:03:12.565766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10619 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n",
      "INFO:tensorflow:Restoring parameters from /home/phil/trained_model/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference results INPUT: Thank you heavenly Father\n",
      "INFO:tensorflow:Inference results OUTPUT: ाााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााा\n",
      "INFO:tensorflow:Inference results INPUT: Jesus loves you\n",
      "INFO:tensorflow:Inference results OUTPUT: ाााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााााा\n",
      "INFO:tensorflow:Elapsed Time: 10.26257\n",
      "INFO:tensorflow:Averaged Single Token Generation Time: 0.0801588\n",
      "INFO:tensorflow:Writing decodes into /home/phil/bible/my.txt.transformer.transformer_base.bible.beam4.alpha0.6.decodes\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATA_DIR=/home/phil/t2t_data\n",
    "OUTDIR=/home/phil/trained_model\n",
    "DECODE_FILE=/home/phil/big_en_hi_small/my.txt\n",
    "\n",
    "PROBLEM=big_en_hi_small\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_base\n",
    "\n",
    "\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "  --data_dir=$DATA_DIR \\\n",
    "  --problem=big_en_hi_small \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_base \\\n",
    "  --output_dir=/home/phil/trained_model \\\n",
    "  --t2t_usr_dir=/home/phil/big_en_hi_small/trainer \\\n",
    "  --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "  --decode_from_file=$DECODE_FILE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
